{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ucUIoKyJdAxb"},"source":["# Пишем свой фреймворк"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"o2DkIzfVdAxd"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["Инициализируем абстрактный класс \"модуль\":"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{},"colab_type":"code","id":"oBVpDMN6dAxs"},"outputs":[],"source":["class Module():\n","    def __init__(self):\n","        self._train = True\n","    \n","    def forward(self, input):\n","        raise NotImplementedError\n","\n","    def backward(self,input, grad_output):\n","        raise NotImplementedError\n","    \n","    def parameters(self):\n","        'Возвращает список собственных параметров.'\n","        return []\n","    \n","    def grad_parameters(self):\n","        'Возвращает список тензоров-градиентов для своих параметров.'\n","        return []\n","    \n","    def train(self):\n","        self._train = True\n","    \n","    def eval(self):\n","        self._train = False"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TV9xeNAndAxv"},"source":["Sequential:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{},"colab_type":"code","id":"0O968yWrdAxx","outputId":"a2d80f27-6958-4c48-e947-0f905db7cbb9"},"outputs":[],"source":["class Sequential(Module):\n","    def __init__ (self, *layers):\n","        super().__init__()\n","        self.layers = layers\n","\n","    def forward(self, input):\n","        for layer in self.layers:\n","            input = layer.forward(input)\n","        self.output = input\n","        return self.output\n","\n","    def backward(self, input, grad_output):\n","        for i in range(len(self.layers)-1, 0, -1):\n","            grad_output = self.layers[i].backward(self.layers[i-1].output, grad_output)        \n","        grad_input = self.layers[0].backward(input, grad_output)\n","        return grad_input\n","      \n","    def parameters(self):\n","        res = []\n","        for l in self.layers:\n","            res += l.parameters()\n","        return res\n","    \n","    def grad_parameters(self):\n","        res = []\n","        for l in self.layers:\n","            res += l.grad_parameters()\n","        return res\n","    \n","    def train(self):\n","        for layer in self.layers:\n","            layer.train()\n","    \n","    def eval(self):\n","        for layer in self.layers:\n","            layer.eval()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"50R6jFr8dAxy"},"source":["Fully-connected layer:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{},"colab_type":"code","id":"4uEjKIasdAx3"},"outputs":[],"source":["class Linear(Module):\n","    def __init__(self, dim_in, dim_out):\n","        super().__init__()\n","        stdv = 1./np.sqrt(dim_in)\n","        self.W = np.random.uniform(-stdv, stdv, size=(dim_in, dim_out))\n","        self.b = np.random.uniform(-stdv, stdv, size=dim_out)\n","        \n","    def forward(self, input):\n","        self.output = np.dot(input, self.W) + self.b\n","        return self.output\n","    \n","    def backward(self, input, grad_output):\n","        self.grad_b = np.mean(grad_output, axis=0)\n","        self.grad_W = np.dot(input.T, grad_output)\n","        self.grad_W /= input.shape[0]\n","        grad_input = np.dot(grad_output, self.W.T)\n","        \n","        return grad_input\n","    \n","    def parameters(self):\n","        return [self.W, self.b]\n","    \n","    def grad_parameters(self):\n","        return [self.grad_W, self.grad_b]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-jLyEmMEdAx7"},"source":["ReLU:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{},"colab_type":"code","id":"HrayJEANdAx7"},"outputs":[],"source":["class ReLU(Module):\n","    def __init__(self):\n","         super().__init__()\n","    \n","    def forward(self, input):\n","        self.output = np.maximum(input, 0)\n","        return self.output\n","    \n","    def backward(self, input, grad_output):\n","        grad_input = np.multiply(grad_output, input > 0)\n","        return grad_input"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["Сравним функцию с реализацией в пайторч:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ReLU custom:\n"," [1 2 0 4 0]\n","ReLU torch:\n"," tensor([1., 2., 0., 4., 0.], grad_fn=<ReluBackward0>)\n","ReLU backward torch:\n"," tensor([1., 1., 0., 1., 0.])\n","ReLU backward custom:\n"," [1. 1. 0. 1. 0.]\n"]}],"source":["input_torch = torch.tensor([1., 2., -3., 4., -7.], requires_grad=True)\n","input_array = np.array([1, 2, -3, 4, -7])\n","\n","layer_custom = ReLU()\n","layer_torch = nn.ReLU()(input_torch)\n","\n","print('ReLU custom:\\n', layer_custom.forward(input_array))\n","print('ReLU torch:\\n', layer_torch)\n","\n","layer_torch.backward(torch.Tensor([1, 1, 1, 1, 1]))\n","print('ReLU backward torch:\\n', input_torch.grad)\n","print('ReLU backward custom:\\n', layer_custom.backward(input_array, np.ones(5)))\n"]},{"cell_type":"markdown","metadata":{},"source":["LeakyReLU:"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{},"colab_type":"code","id":"OMWPFjxldAx8"},"outputs":[],"source":["class LeakyReLU(Module):\n","    def __init__(self, slope=0.03):\n","        super().__init__()\n","        self.slope = slope\n","\n","    def forward(self, input):\n","        self.output = np.maximum(input, np.multiply(self.slope, input))\n","        return self.output\n","\n","    def backward(self, input, grad_output):\n","        grad_input = np.multiply(grad_output, input > 0) + np.multiply(self.slope, np.multiply(grad_output, input < 0))\n","        return grad_input"]},{"cell_type":"markdown","metadata":{},"source":["Сравнение функций для LeakyReLU:"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["LeakyReLU custom:\n"," [ 1.    2.   -0.15  4.   -0.35]\n","LeakyReLU torch:\n"," tensor([ 1.0000,  2.0000, -0.1500,  4.0000, -0.3500],\n","       grad_fn=<LeakyReluBackward0>)\n","LeakyReLU backward torch:\n"," tensor([1.0000, 1.0000, 0.0500, 1.0000, 0.0500])\n","LeakyReLU backward custom:\n"," [1.   1.   0.05 1.   0.05]\n"]}],"source":["input_torch = torch.tensor([1., 2., -3., 4., -7.], requires_grad=True)\n","input_array = np.array([1, 2, -3, 4, -7])\n","\n","layer_custom = LeakyReLU(0.05)\n","layer_torch = nn.LeakyReLU(0.05)(input_torch)\n","\n","print('LeakyReLU custom:\\n', layer_custom.forward(input_array))\n","print('LeakyReLU torch:\\n', layer_torch)\n","\n","layer_torch.backward(torch.Tensor([1, 1, 1, 1, 1]))\n","print('LeakyReLU backward torch:\\n', input_torch.grad)\n","print('LeakyReLU backward custom:\\n', layer_custom.backward(input_array, np.ones(5)))\n"]},{"cell_type":"markdown","metadata":{},"source":["Sigmoid:"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{},"colab_type":"code","id":"LUag-tekdAx-"},"outputs":[],"source":["class Sigmoid(Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, input):\n","        self.output = 1 / (1 + np.exp(-input))\n","        return self.output\n","    \n","    def backward(self, input, grad_output):\n","        grad_input = np.multiply(self.forward(input), 1 - self.forward(input))\n","        return grad_input"]},{"cell_type":"markdown","metadata":{},"source":["Сравнение функций для Sigmoid:"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["custom:\n"," [[0.63575811 0.28377758]\n"," [0.63575811 0.28377758]]\n","torch:\n"," tensor([[0.6358, 0.2838],\n","        [0.6358, 0.2838]], grad_fn=<SigmoidBackward>)\n","Sigmoid backward torch:\n"," tensor([[0.2316, 0.2032],\n","        [0.2316, 0.2032]])\n","Sigmoid backward custom:\n"," [[0.23156973 0.20324786]\n"," [0.23156973 0.20324786]]\n"]}],"source":["input_torch = torch.tensor([[ 0.5570, -0.9258], [ 0.5570, -0.9258]], requires_grad=True)\n","input_array = np.array([[ 0.5570, -0.9258], [ 0.5570, -0.9258]])\n","\n","layer_custom = Sigmoid()\n","layer_torch = nn.Sigmoid()(input_torch)\n","\n","print('custom:\\n', layer_custom.forward(input_array))\n","print('torch:\\n', layer_torch)\n","\n","layer_torch.backward(torch.Tensor([[1, 1], [1, 1]]))\n","print('Sigmoid backward torch:\\n', input_torch.grad)\n","print('Sigmoid backward custom:\\n', layer_custom.backward(input_array, np.ones((2, 2))))"]},{"cell_type":"markdown","metadata":{},"source":["Softmax:"]},{"cell_type":"markdown","metadata":{},"source":["Нам не надо писать backward для софтмакса, потому что он уже учтен в backward'e кросс-энтропии:"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{},"colab_type":"code","id":"QS-6BIe6dAx_"},"outputs":[],"source":["class SoftMax(Module):\n","    def __init__(self):\n","         super().__init__()\n","    \n","    def forward(self, input):\n","        sub_input = np.subtract(input, input.max(axis=1, keepdims=True))\n","        self.output = np.exp(sub_input) / np.sum(np.exp(sub_input), axis=1, keepdims=True)\n","        return self.output\n","    \n","    def backward(self, input, grad_output):\n","        return grad_output"]},{"cell_type":"markdown","metadata":{},"source":["Сравнение функций для Softmax:"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["custom:\n"," [[0.2157302  0.32267532 0.1661519  0.13603299 0.15940959]\n"," [0.1436334  0.16224488 0.22408223 0.25565338 0.21438612]\n"," [0.1846925  0.21401673 0.17103187 0.18323973 0.24701917]]\n","torch:\n"," tensor([[0.2157, 0.3227, 0.1662, 0.1360, 0.1594],\n","        [0.1436, 0.1622, 0.2241, 0.2557, 0.2144],\n","        [0.1847, 0.2140, 0.1710, 0.1832, 0.2470]], dtype=torch.float64,\n","       grad_fn=<SoftmaxBackward>)\n","\n","Softmax backward torch:\n"," tensor([[-0.0685, -0.0214,  0.0471,  0.0611, -0.0183],\n","        [-0.0076, -0.0775,  0.0341,  0.1054, -0.0545],\n","        [ 0.0477,  0.0150,  0.0712, -0.0611, -0.0729]], dtype=torch.float64)\n","Softmax backward custom:\n"," [[-0.06848399 -0.02139871  0.04710827  0.06105249 -0.01827806]\n"," [-0.007614   -0.07746672  0.03412195  0.10541092 -0.05445214]\n"," [ 0.04769257  0.01500802  0.07120759 -0.06105651 -0.07285167]]\n"]}],"source":["input_array = np.random.uniform(0, 1, (3, 5))\n","grad = np.random.uniform(0, 1, (3, 5))\n","input_tensor = torch.tensor(input_array, requires_grad=True)\n","layer_custom = SoftMax()\n","layer_torch = nn.functional.softmax(input_tensor)\n","\n","print('custom:\\n', layer_custom.forward(input_array))\n","print('torch:\\n', layer_torch)\n","\n","# Не было необходимости писать беквард для софтмакса, но давайте все равно его протестим:\n","def softmax_backward(softmax, grad_out):\n","    result = []\n","    for i in range(grad_out.shape[0]):\n","        result.append(np.dot((np.diagflat(softmax[i]) - np.dot(softmax[i].T.reshape(-1,1), softmax[i].reshape(1,-1))), grad_out[i].reshape(-1, 1)))\n","    return np.array(result).reshape(grad.shape)\n","\n","layer_torch.backward(torch.Tensor(grad))\n","print()\n","print('Softmax backward torch:\\n', input_tensor.grad)\n","print('Softmax backward custom:\\n', softmax_backward(layer_custom.forward(input_array), grad))\n"]},{"cell_type":"markdown","metadata":{},"source":["Dropout:"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["class Dropout(Module):\n","    def __init__(self, p=0.5):\n","        super().__init__()\n","        self.p = p\n","        self.mask = None\n","\n","    def forward(self, input):\n","        if self._train:\n","            self.mask = np.random.binomial(1, (1-self.p), size=input.shape)\n","            self.output = self.mask * input\n","        else:\n","            self.output = input * (1-self.p)\n","        return self.output\n","\n","    def backward(self, input, grad_output):\n","        if self._train:\n","            self.grad_input = self.mask * grad_output\n","        else:\n","            self.grad_input = grad_output\n","        return self.grad_input"]},{"cell_type":"markdown","metadata":{},"source":["BatchNorm:"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["class BatchNorm(Module):\n","    def __init__(self, num_features):\n","        super().__init__()\n","        self.gamma = np.ones(num_features)\n","        self.beta = np.zeros(num_features)\n","        self.mu = np.zeros(shape=num_features)\n","        self.sigma = np.ones(shape=num_features)\n","        self.momentum = 0.9\n","        self.eps = 1e-5\n","\n","    def forward(self, input):\n","        if self._train:\n","            mu_new = np.mean(input, axis=0)\n","            sigma_new = np.mean((input - mu_new) ** 2, axis=0)\n","            self.mu = self.momentum * self.mu + (1 - self.momentum) * mu_new\n","            self.sigma = self.momentum * self.sigma + (1 - self.momentum) * sigma_new\n","            input_norm = (input - mu_new) / np.sqrt(sigma_new + self.eps)\n","            self.output = self.gamma * input_norm + self.beta\n","        else:\n","            input_norm = (input - self.mu) / np.sqrt(self.sigma + self.eps)\n","            self.output = self.gamma * input_norm + self.beta\n","        return self.output\n","\n","    def backward(self, input, grad_output):\n","        if self._train:\n","            mu = np.mean(input, axis=0)\n","            sigma = np.mean((input - mu) ** 2, axis=0)\n","            input_norm = (input - mu) / np.sqrt(sigma + self.eps)\n","            t = 1. / np.sqrt(sigma + self.eps)\n","            m = input.shape[0]\n","            self.grad_gamma = np.sum(grad_output * input_norm, axis=0)\n","            self.grad_beta = np.sum(grad_output, axis=0)\n","            grad_x = (self.gamma * t / m) * (m * grad_output - t ** 2 * (input - mu) * np.sum(grad_output * (input - mu), axis=0) - np.sum(grad_output, axis=0))\n","            grad_input = grad_x\n","\n","        else:\n","            input_norm = (input - self.mu) / np.sqrt(self.sigma + self.eps)\n","            t = 1. / np.sqrt(self.sigma + self.eps)\n","            m = input.shape[0]\n","            self.grad_gamma = np.sum(grad_output * input_norm, axis=0)\n","            self.grad_beta = np.sum(grad_output, axis=0)\n","            grad_x = (self.gamma * t / m) * (m * grad_output - t ** 2 * (input - self.mu) * np.sum(grad_output * (input - self.mu), axis=0) - np.sum(grad_output, axis=0))\n","            grad_input = grad_x\n","\n","        return grad_input\n","\n","    def parameters(self):\n","        return [self.gamma, self.beta]\n","\n","    def grad_parameters(self):\n","        return [self.grad_gamma, self.grad_beta]"]},{"cell_type":"markdown","metadata":{},"source":["Сравнение для BatchNorm:"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["custom:\n"," [[-0.31578416  0.2206727   0.55805683  0.53968656  0.26611346]\n"," [ 1.61705005  0.22198747  0.06159324  1.19194663  0.13319016]\n"," [ 2.05434728  0.18678124  1.21635592 -0.17682622  1.08032107]\n"," [-0.33395895 -0.65781593 -0.55126923  0.87074119  0.73552126]\n"," [ 0.85521311  0.96148813 -1.66470075  0.24545448  2.28932834]]\n","torch:\n"," tensor([[-0.3158,  0.2207,  0.5581,  0.5397,  0.2661],\n","        [ 1.6171,  0.2220,  0.0616,  1.1919,  0.1332],\n","        [ 2.0543,  0.1868,  1.2164, -0.1768,  1.0803],\n","        [-0.3340, -0.6578, -0.5513,  0.8707,  0.7355],\n","        [ 0.8552,  0.9615, -1.6647,  0.2455,  2.2893]],\n","       grad_fn=<SliceBackward>)\n","--------------\n"]}],"source":["input_tensor = torch.randn(64, 5)\n","grad_tensor = torch.randn(64, 5)\n","\n","layer_custom = BatchNorm(5)\n","layer_torch = nn.BatchNorm1d(5)\n","\n","layer_torch.train()\n","layer_custom._train = True\n","\n","print('custom:\\n', layer_custom.forward(input_tensor.detach().numpy())[:5])\n","print('torch:\\n', layer_torch(input_tensor)[:5])\n","\n","print('--------------')"]},{"cell_type":"markdown","metadata":{},"source":["Абстрактный класс для Criterion:"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{},"colab_type":"code","id":"COX7VXtUdAyC"},"outputs":[],"source":["class Criterion():        \n","    def forward(self, input, target):\n","        raise NotImplementedError\n","\n","    def backward(self, input, target):\n","        raise NotImplementedError"]},{"cell_type":"markdown","metadata":{},"source":["MSE:"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{},"colab_type":"code","id":"RLZXaG3OdAyD"},"outputs":[],"source":["class MSE(Criterion):\n","    def forward(self, input, target):\n","        self.output = np.sum(np.power(input - target, 2)) / input.shape[0]\n","        return self.output\n"," \n","    def backward(self, input, target):\n","        self.grad_output  = (input - target) * 2 / input.shape[0]\n","        return self.grad_output"]},{"cell_type":"markdown","metadata":{},"source":["Сравнение функций для MSE:"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch:  tensor(1.7903, dtype=torch.float64, grad_fn=<MseLossBackward>)\n","custom:  1.7903148627563954\n","Gradient torch:\n"," tensor([[ 1.1027],\n","        [ 0.0807],\n","        [ 0.3511],\n","        [-0.1863],\n","        [-0.2276]], dtype=torch.float64)\n","Gradient custom:\n"," [[ 1.10272365]\n"," [ 0.08073183]\n"," [ 0.35105221]\n"," [-0.18627612]\n"," [-0.22759252]]\n"]}],"source":["predict = np.random.randn(5, 1)\n","labels = np.random.randn(5, 1)\n","\n","pred_tensor = torch.tensor(predict, requires_grad=True)\n","labels_tensor = torch.tensor(labels)\n","\n","MSE_torch = nn.functional.mse_loss(pred_tensor, labels_tensor)\n","\n","print('torch: ', MSE_torch)\n","print('custom: ', MSE().forward(predict, labels))\n","\n","MSE_torch.backward()\n","print('Gradient torch:\\n', pred_tensor.grad)\n","print('Gradient custom:\\n', MSE().backward(predict, labels))"]},{"cell_type":"markdown","metadata":{},"source":["CrossEntropy:"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{},"colab_type":"code","id":"3ckg7SdddAyF"},"outputs":[],"source":["class CrossEntropy(Criterion):\n","    def __init__(self):\n","        super().__init__()\n","        \n","    def forward(self, input, target): \n","        eps = 1e-9\n","        input_clamp = np.clip(input, eps, 1 - eps)               \n","        self.output = -1 * np.sum( np.multiply( np.log( SoftMax().forward(input_clamp) ), target ) ) / input_clamp.shape[0]\n","        return self.output\n","\n","    def backward(self, input, target):\n","        eps = 1e-9\n","        input_clamp = np.clip(input, eps, 1 - eps)\n","\n","        grad_input = (SoftMax().forward(input_clamp) - target)\n","        \n","        # Чтобы градиент сходился с градиентом торча, раскомментить:           \n","        # grad_input = (SoftMax().forward(input_clamp) - target) / input_clamp.shape[0]\n","\n","        return grad_input"]},{"cell_type":"markdown","metadata":{},"source":["Cравнение функций для CrossEntropy:"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","predict:\n"," [[0.69067852 0.43224745 0.57056269 0.72160854 0.13139046]\n"," [0.90132604 0.56409229 0.94320448 0.01381289 0.75059249]\n"," [0.59322367 0.16130528 0.33159224 0.68269917 0.46577442]]\n","labels:\n"," [[0 0 1 0 0]\n"," [1 0 0 0 0]\n"," [0 0 0 0 1]]\n","torch:  tensor(1.5236, dtype=torch.float64, grad_fn=<NllLossBackward>)\n","custom:  1.5235587728685556\n","Gradient torch:\n"," tensor([[ 0.0782,  0.0604, -0.2640,  0.0807,  0.0447],\n","        [-0.2506,  0.0591,  0.0863,  0.0341,  0.0712],\n","        [ 0.0759,  0.0493,  0.0584,  0.0830, -0.2665]], dtype=torch.float64)\n","Gradient custom:\n"," [[ 0.23462562  0.18119251 -0.79192985  0.24199599  0.13411573]\n"," [-0.75175415  0.17718348  0.25886275  0.10219742  0.2135105 ]\n"," [ 0.2276351   0.1477949   0.17523229  0.24894187 -0.79960415]]\n"]}],"source":["predict = np.random.uniform(0, 1, (3, 5))\n","labels = np.array([[0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 1]])\n","print()\n","print('predict:\\n', predict)\n","print('labels:\\n', labels)\n","\n","pred_tensor = torch.tensor(predict, requires_grad=True)\n","labels_tensor = torch.tensor([2, 0, 4], dtype=torch.long)\n","\n","CE_torch = nn.functional.cross_entropy(pred_tensor, labels_tensor)\n","\n","print('torch: ', CE_torch)\n","print('custom: ', CrossEntropy().forward(predict, labels))\n","\n","CE_torch.backward()\n","print('Gradient torch:\\n', pred_tensor.grad)\n","print('Gradient custom:\\n', CrossEntropy().backward(predict, labels))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"hw_framework.ipynb","provenance":[]},"interpreter":{"hash":"f153fec09f1ca502b2c3ed8d73bff2e5dbba817797492f5a49591745003c6d18"},"kernelspec":{"display_name":"Python 3.9.1 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
